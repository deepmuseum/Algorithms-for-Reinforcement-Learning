{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Temporal-Difference Learning.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z8h6iEqcZ-Zb",
        "outputId": "8955cdd7-2c92-4a0c-ab39-ede6bc428d3a"
      },
      "source": [
        "!git clone https://github.com/deepmuseum/Algorithms-for-Reinforcement-Learning.git"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'Algorithms-for-Reinforcement-Learning' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8Isn4xnirQH"
      },
      "source": [
        "import sys\n",
        "sys.path.insert(0, './Algorithms-for-Reinforcement-Learning/envs')\n",
        "import numpy as np\n",
        "from scipy.special import softmax \n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "import math\n",
        "from test_env import ToyEnv1"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4zZ6wz3ASI9"
      },
      "source": [
        "## The environment: a quick start"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZ8ipBrm_hel",
        "outputId": "11bc8891-e416-4f78-b002-51625dfc88a9"
      },
      "source": [
        "env = ToyEnv1(gamma=0.99)\n",
        "\n",
        "# Useful attributes\n",
        "print(\"Set of states:\", env.states)\n",
        "print(\"Set of actions:\", env.actions)\n",
        "print(\"Number of states: \", env.Ns)\n",
        "print(\"Number of actions: \", env.Na)\n",
        "print(\"P has shape: \", env.P.shape)  # P[s, a, s'] = env.P[s, a, s']\n",
        "print(\"discount factor: \", env.gamma)\n",
        "print(\"\")\n",
        "\n",
        "# Usefult methods\n",
        "state = env.reset() # get initial state\n",
        "print(\"initial state: \", state)\n",
        "print(\"reward at (s=1, a=3,s'=2): \", env.reward_func(1,3,2))\n",
        "print(\"\")\n",
        "\n",
        "# A random policy\n",
        "policy = np.random.randint(env.Na, size = (env.Ns,))\n",
        "print(\"random policy = \", policy)\n",
        "\n",
        "# Interacting with the environment\n",
        "print(\"(s, a, s', r):\")\n",
        "for time in range(4):\n",
        "    action = policy[state]\n",
        "    next_state, reward, done, info = env.step(action)\n",
        "    print(state, action, next_state, reward)\n",
        "    if done:\n",
        "        break\n",
        "    state = next_state\n",
        "print(\"\")\n",
        "print(env.R.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Set of states: [0, 1, 2]\n",
            "Set of actions: [0, 1]\n",
            "Number of states:  3\n",
            "Number of actions:  2\n",
            "P has shape:  (3, 2, 3)\n",
            "discount factor:  0.99\n",
            "\n",
            "initial state:  0\n",
            "reward at (s=1, a=3,s'=2):  1.0\n",
            "\n",
            "random policy =  [0 0 1]\n",
            "(s, a, s', r):\n",
            "0 0 1 0.0\n",
            "1 0 2 1.0\n",
            "\n",
            "(3, 2, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKK_dulMBtsO"
      },
      "source": [
        "## Tabular TD(0) for estimating $v_π$\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "Input: the policy π to be evaluated\n",
        "Initialize V (s) arbitrarily (e.g., V (s) = 0, for all s ∈ S + )\n",
        "Repeat (for each episode):\n",
        "  Initialize S\n",
        "  Repeat (for each step of episode):\n",
        "    A ← action given by π for S\n",
        "    Take action A, observe R, S'\n",
        "    V (S) ← V (S) + α (R + γV (S') − V (S))\n",
        "    S ← S'\n",
        "  until S is terminal\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqjt_8VuBeu8",
        "outputId": "8060f4e6-f719-40e8-ed81-9b3ecfa3b4a0"
      },
      "source": [
        "# A random policy\n",
        "policy = np.random.randint(env.Na, size = (env.Ns,))\n",
        "\n",
        "V=np.zeros(3)\n",
        "n_episodes=1000\n",
        "alpha=0.1\n",
        "gamma=env.gamma\n",
        "for episode in range(n_episodes):\n",
        "  state=env.reset()\n",
        "  done=False\n",
        "  while not done:\n",
        "    action=policy[state]\n",
        "    next_state, reward, done, info = env.step(action)\n",
        "    V[state]+=alpha*(reward+gamma*V[next_state]-V[state])\n",
        "    state=next_state\n",
        "\n",
        "print('Predicted Value Function: ', V)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicted Value Function:  [0.99043148 0.96781512 0.        ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VKhepyrKO_5"
      },
      "source": [
        "## Sarsa (on-policy TD control) for estimating $Q ≈ q^∗$\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "Initialize Q(s, a), for all s ∈ S, a ∈ A(s), arbitrarily, and Q(terminal-state, ·) = 0\n",
        "Repeat (for each episode):\n",
        "  Initialize S\n",
        "  Choose A from S using policy derived from Q (e.g., eps-greedy)\n",
        "  Repeat (for each step of episode):\n",
        "    Take action A, observe R, S'\n",
        "    Choose A' from S' using the policy derived from Q (e.g. eps-greedy)\n",
        "    Q(S, A) ← Q(S, A) + α (R + γQ(S', A') − Q(S, A))\n",
        "    S ← S' ; A ← A' ;\n",
        "  until S is terminal\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ngazOolLCVE",
        "outputId": "886dfc7f-d4b6-4461-dcac-4323e39b1acc"
      },
      "source": [
        "def eps_greedy(state, Q, epsilon):\n",
        "  if np.random.uniform()<=epsilon:\n",
        "    return np.random.randint(0,env.Na)\n",
        "  else:\n",
        "    return np.argmax(Q[state])\n",
        "\n",
        "Q=np.zeros((env.Ns,env.Na))\n",
        "n_episodes=100\n",
        "aplha=0.1\n",
        "epsilon=0.1\n",
        "gamma=env.gamma\n",
        "for episode in range(n_episodes):\n",
        "  state=env.reset()\n",
        "  done=False\n",
        "  while not done:\n",
        "    action=eps_greedy(state,Q,epsilon)\n",
        "    next_state, reward, done, info = env.step(action)\n",
        "    next_action=eps_greedy(next_state,Q,epsilon)\n",
        "    Q[state,action]+=alpha*(reward+gamma*Q[next_state,next_action]-Q[state,action])\n",
        "    state,action=next_state,next_action\n",
        "\n",
        "print('Optimal Q-function: ')\n",
        "print(Q)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Optimal Q-function: \n",
            "[[0.90571261 0.45590375]\n",
            " [0.87275019 0.65371894]\n",
            " [0.         0.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56WnpZFERw1k"
      },
      "source": [
        "## Q-learning (off-policy TD control) for estimating $π ≈ π^∗$\n",
        "\n",
        "\n",
        "```\n",
        "Initialize Q(s, a), for all s ∈ S, a ∈ A(s), arbitrarily, and Q(terminal-state, ·) = 0\n",
        "Repeat (for each episode):\n",
        "  Initialize S\n",
        "  Repeat (for each step of episode):\n",
        "    Choose A from S using policy derived from Q (e.g., eps-greedy)\n",
        "    Take action A, observe R, S'\n",
        "    Q(S, A) ← Q(S, A) + α( R + γ max_a Q(S', a) − Q(S, A))\n",
        "    S ← S'\n",
        "  until S is terminal\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T11kZSsVNfuk",
        "outputId": "9c4ce820-38b8-4923-e141-24f307f894c0"
      },
      "source": [
        "Q=np.zeros((env.Ns,env.Na))\n",
        "n_episodes=100\n",
        "aplha=0.1\n",
        "epsilon=0.1\n",
        "gamma=env.gamma\n",
        "for episode in range(n_episodes):\n",
        "  state=env.reset()\n",
        "  done=False\n",
        "  while not done:\n",
        "    action=eps_greedy(state,Q,epsilon)\n",
        "    next_state, reward, done, info = env.step(action)\n",
        "    next_action=np.argmax(Q[next_state])\n",
        "    Q[state,action]+=alpha*(reward+gamma*Q[next_state,next_action]-Q[state,action])\n",
        "    state=next_state\n",
        "\n",
        "print('Optimal Q-function: ')\n",
        "print(Q)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Optimal Q-function: \n",
            "[[0.95962607 0.46161132]\n",
            " [0.96214882 0.45946134]\n",
            " [0.         0.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ClGW9TwVST9"
      },
      "source": [
        "## Double Q-learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bz2NlbgTU-6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
